---
title: "The Imitation Game"
date: 2022-10-15T00:01:12-07:00
draft: false
---

Arthur C. Clarke, the famous science fiction writer, once said that "Any sufficiently advanced technology is indistinguishable from magic." This statement is especially true in the field of Artificial Intelligence. In the recent years, we have seen AI [beat](https://www.deepmind.com/research/highlighted-research/alphago) human champions at game of Go. We have seen them creating art and winning [prizes](https://www.nytimes.com/2022/09/02/technology/ai-artificial-intelligence-artists.html). There's even an [essay](https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3) written by AI published on The Guardian trying to convince us "robots come in peace". It seem that human, using machine learning algorithms, has acquired a power indistinguishable from magic. We seem to be closer and closer to being able to create intelligent beings without the help from God or millions of years of evolution. With the "magic power" of AI, comes questions and doubts from the public: "Does AI art has the same value as human art?", "Does AI generated texts represent what AI thinks?", and most importantly "Are we going to be replaced by AI?" At the center of all these confusion lies one fundamental problem of wether computer programs can be considered to possess intelligence or consciousness. It is true that the AI programs can solve difficult problems and exhibit complex behaviors, but they do it with computer algorithms, statistic models, and math equations. Can these combination of algorithms, models, and equations be considered the same as intelligence? It seems that to determine whether programs can have intelligence, we must first find a concrete definition of intelligence itself. This won't be easy task since the nature of intelligence has been debated in the field of philosophy for centuries without a definitive answer. However, we can find a short cut for this questing using one fact we do know for certain about intelligence ---- that human in general possesses it. Thus, in this paper, we will show AI will eventually gain true human-like intelligence by doing an imitation of human behaviors. 
 
To examine the potential capabilities of artificial intelligence, we must first understand the goal of this branch of computer science. In the book, "[Artificial Intelligence: A Modern Approach](http://aima.cs.berkeley.edu/)", the authors provide two ways of measuring success of an AI agent: "human performance" and "ideal performance". "Human performance" cares about how good an agent can simulate a human's reasoning process and behaviors, whereas  "ideal performance" measure success by judging if a system makes the optimal decision given what it knows. One of the first lessons a C.S. student learn in an AI class is that the goal of artificial intelligence is to build rational systems that optimize "ideal performance". Under this direction, computer scientists have indeed created many successful AI agents to perform various tasks. For example, "[Shakey the Robot](https://ai.stanford.edu/~nilsson/OnlinePubs-Nils/shakey-the-robot.pdf)" was built in Stanford in the 60s to be the first mobile robot with the ability to perceive and reason about its surroundings. It navigates around obstacles by predicting the length of potential paths with a mathematical model and choosing the shortest one. Another more significant example is Deep Blue, the machine that beats world chess champion Garry Kasparov in 1997. Similar to Shakey's navigation method, Deep Blue estimate the outcomes of each possible move and choose the best move based on its estimation. As successful as these agents are, such a rational system can hardly be considered as "intelligent", at least in the sense of "human intelligence". A system build under this goal will typically operate under a mathematical model to calculate and optimize the outcome of its decision. To construct such a mathematical model, the problem scope must be well defined. Thus, an agent can not generalize beyond what it is designed to do. Shakey can not produce a single chess move and Deep Blue will never move to the patio to have a smoke in the middle of a chess game. Moreover, some intelligent computer scientists must be behind it to design the system, so the intelligence of the agent can hardly be considered its own intelligence since all the important reasonings are already done by the people backstage. In terms of intelligence, these "rational systems" are no better than a toaster which decide when to pop the toast out based on its user's settings. This is not to say these systems cannot evolve or learn and are only capable of doing exactly what they are instructed to do. In fact a majority of the machine learning systems are designed under the principle of maximizing "optimal performance". However, these "learnings" are mostly done to measure parameters crucial to the system's math model from the data. The agent still cannot evolve beyond the model pre-defined by human designers and create its own method of reasoning. After all, a toaster that can decide when to pop the toast by "learning" the temperature of the toast is still just a toaster. 

It seems that the goal of maximizing "ideal performance" is at odds with creating "intelligence", which strengthens the claim that computer programs cannot ever be intelligent like humans. However, this is only true for AI in a more traditional sense. In recent years, the line between "idea performance" and "human performance" has become less and less well defined. As researchers move from problems like simple navigation and calculating chess moves to more complex problems like machine translation or text generation, it gets increasingly harder to design a good mathematical model to find the optimal decisions. This  motivates computer scientists to try solving these problems by maximizing "human performance" first. By imitating the decisions made by humans, the system's output may not always be optimal, however, such a system can at least solve many difficult problems as good as human does. One of the most influential instance of such system is AlphaGo who defeated human Go champion Lee Sedol in 4 of 5 games. Unlike chess, evaluating all possible outcomes of every in a game of Go is far beyond the computation ability of even the best supercomputer, which is why many believe AI cannot beat human at game of Go for at least a few decades. From the paper published by DeepMind, we learn that Alpha-go's solution is a hybrid of "ideal performance" approach and "human performance" approach.  Like the case of Deep Blue, researchers at DeepMind still designed a intelligent algorithm to evaluate each  possible move and find the best one. What's different is that on the foundation of all the algorithms is a system that learn from thousands of human Go games to imitate the ways human masters make their decisions. What sets AlphaGo apart from DeepBlue is that it is no longer completely dependent on the math model set up by its creator. AlphaGo, at least its foundation, solves the problem of playing Go by mimicking the moves of Go masters. We must not that this is not merely a copy of Go master's intelligence, since AlphaGo can still make good decisions even in situations it has never seen in its data. After AlphaGo, we see even more success in AI by maximizing "human performance". The most notable one is [GPT-3](https://arxiv.org/pdf/2005.14165.pdf), which is a language generation model that can finish a sentence or answer questions. It is trained by imitating millions and billions of texts collected from the internet and the resulting model could not only sound like human but also produce factual answer, faithful translation, and complex arguments. It even generated a full article that ended up being publish on [The Guardian](https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3). Although texts or behaviors produced by systems like GPT-3 is cannot be completely indistinguishable from those generated by human yet, but the possibility of a human-like system seems significant enough to be seriously considered. We know that such a system would be fundamentally different and probably more capable than the traditional AI methods based on the principle of "ideal performance". However, it is not obvious whether by imitating human intelligence a computer program can become intelligent. 

Even though the technologies behind AlphaGo and GPT-3 only came to existence in the last 20 years, the idea of creating machine intelligence by imitating intelligent beings as far back as the 50s when general computers had just moved on from being a theoretical possibility to real working machines. One of the earliest work on this idea is the 1950 paper "[Computing Machinery and Intelligence](file:///Users/humanmcperson/Zotero/storage/4PE4HUR3/986238.html)" by Alan Turing. In his paper, Turing described an experiment to determine wether a machine can think called the Imitation Game which is commonly known today as "the Turing Test". [Stanford Encyclopedia of Philosophy](https://plato.stanford.edu/entries/turing-test/) has a great summary of the "the Imitation Game" proposed by Turing:

> Suppose that we have a person, a machine, and an interrogator. The interrogator is in a room separated from the other person and the machine. The object of the game is for the interrogator to determine which of the other two is the person, and which is the machine. The interrogator knows the other person and the machine by the labels ‘X’ and ‘Y’—but, at least at the beginning of the game, does not know which of the other person and the machine is ‘X’—and at the end of the game says either ‘X is the person and Y is the machine’ or ‘X is the machine and Y is the person’. The interrogator is allowed to put questions to the person and the machine of the following kind: “Will X please tell me whether X plays chess?” Whichever of the machine and the other person is X must answer questions that are addressed to X. The object of the machine is to try to cause the interrogator to mistakenly conclude that the machine is the other person; the object of the other person is to try to help the interrogator to correctly identify the machine. 

Turing stated that this "Imitation Game" has the advantage of "drawing a fairly sharp line between the physical and the intellectual capacities of a man". I would allow as to examine the intelligence of machines independent of all other factors. Furthermore, since the questions the interrogator asks is limitless, we can introduce "almost any one of the fields of human endeavor that we wish to include". Turing himself provided some examples of questions and answers we could use in this experiment:
> Q: Please write me a sonnet on the subject of the Forth Bridge. 

> A: Count me out on this one. I never could write poetry.

> Q: Add 34957 to 70764

> A: (Pause about 30 seconds and then give as answer) 105621. 

> Q: Do you play chess?

> A: Yes.

> Q : I have K at my K1, and no other pieces. You have only K at K6 and R at R1. It is your move. What do you play?

> A : (After a pause of 15 seconds) R-R8 mate.

Turing believes that the question of whether machines can think is "too meaningless to deserve discussion". A more accurate form of this question should be "can machines do well in the imitation game?". This makes sense since, as we argued before, the meaning of "thinking" or "intelligence" is not well defined enough to determine if such terms can be applied to machines. However, in his original paper, Turing did not provide any argument on why existence of intelligence can be shown by "doing well in the imitation game", which creates an opening for counterarguments against the validity of "the imitation game"

One argument against the Turing test (or "the imitation game") is that seemingly good imitation of human can be engineered by using some simple but clever strategies. In fact, many AI agents that perform well in real-life Turing test do not have complex understanding, reasoning, or learning abilities. They get their good performance by exploiting the weaknesses of the Turing test. One of these programs is "[Eugene Goostman](https://www.zdnet.com/article/computer-chatbot-eugene-goostman-passes-the-turing-test/)" which successfully convinced 33% of its judges that it was human at a Turing test contest in 2014. Even though the judges were not completely fooled by "Eugene Goostman", it can still be considered as a good performance since Turing himself saw 30% as a good benchmark and predicted that by the end of 20th century some program should be able to reach that performance. "Eugene Goostman" indeed passes that benchmark, but looking at its strategy, it't not difficult to see in fact it does not possess anything that can be considered as intelligence. Eugene is intensionally prorated as a 13-year-old boy from Odessa, Ukraine. In the developer's opinion, 13 year old is an age "not too old to know everything and not too young to know nothing". This gives Eugene an excuse to not being able to answer question like the chess move question or poetry question in Turing's example. Eugene also uses its Ukrainian persona to induces people who "converse" with him to forgive minor grammatical errors in his responses. Moreover, Eugene tends to use humor and personality quirks to misdirect the judges when facing a question it can't answer. We can see this strategy in the following [conversation](https://scottaaronson.blog/?p=1858) between Eugene and computer scientist Scott Aaronson:
> Scott: How many legs does a camel have?

> Eugene: Something between 2 and 4. Maybe, three? :-))) By the way, I still don’t know your specialty – or, possibly, I’ve missed it?

Clearly, even though Eugene have no idea what a camel is or how many legs animals should have, it is still able to give a somewhat human-like answer to trick the judges. This seems to be a good indication that the Turing test is fundamentally flawed. However, we must take into account the limitations when implementing the Turing test in real life. One of them being that human beings are not perfect interrogators in the Turing test setting. Although we are good at acting like human, determining wether a behavior is human-like is a completely different skill. This especially applies to Eugene's success. In the contests where it got the 33% score, most of the judges were not experts in the field of interrogation or artificial intelligence. Once Eugene is questioned by a professional computer scientist like Scott Aronson, its inability to produce reasonable answers tends to quickly become obvious. Another limitation is that in real-life Turing test the questions being asked mostly are short and relatively simple ones and are limited to texts form. We cannot provide Eugene a documentary on camels first and then ask it "How many legs does a camel have". This makes it even harder to determine whether the wrong answer is due to being a machine or simply never knowing about camels. Such limitations of real-life Turing test allows AI agents to perform well without achieving the level of imitation that Turing test requires theoretically.

Another opposition to the Turing test directly questions wether a perfect imitation of intelligent conversation can be considered as equivalent to intelligent itself. To reject Turing test style imitation as a form of intelligent, the problem of how to define intelligence still must be avoided, since without a universally agreed definition we cannot have a universally agreed decision on what is not intelligent. Such an argument is indeed avoided in John Searle's "Chinese Room Argument" which points out that an AI imitation of intelligent speech and behavior is not the same as imitation of human intelligence. The core of "Chinese Room Argument" is a thought experiment which is first introduced in his 1980 essay "[Is Brian's Mind a Computer Program?](https://www.cs.princeton.edu/courses/archive/spr06/cos116/Is_The_Brains_Mind_A_Computer_Program.pdf)". The thought experiment is summarized in the [Stanford Encyclopedia of Philosophy](https://plato.stanford.edu/entries/chinese-room/) as 

> Imagine a native English speaker who knows no Chinese locked in a room full of boxes of Chinese symbols (a data base) together with a book of instructions for manipulating the symbols (the program). Imagine that people outside the room send in other Chinese symbols which, unknown to the person in the room, are questions in Chinese (the input). And imagine that by following the instructions in the program the man in the room is able to pass out Chinese symbols which are correct answers to the questions (the output). The program enables the person in the room to pass the Turing Test for understanding Chinese but he does not understand a word of Chinese.

In his essay, Searle built upon this thought experiment and proposed the following axioms:

> 1. Computer programs are syntactic
> 2. Human minds have mental semantics
> 3. Syntax by itself is neither constitute of nor sufficient for semantics

In his essay, Searle did not give a clear definition of "syntactic" and "semantics". He distinguished the two concept by saying that the Chinese symbols themselves are "syntactic" and "semantics" are the "language understanding" in our mind. From these three axioms, he concluded that "Program are neither constitutive of nor sufficient for semantics". This confusion is a direct argument against what he called "Strong AI" hypothesis which claims that "thinking is merely the manipulation of formal symbols and that is exactly what the computer does". We must note that the "Chinese Room Argument" is proposed in the 1980s when AI technologies was in their infancy. Up till that point, the most prominent form of AI had been "[expert system](https://books.google.com/books/about/Expert_Systems.html?id=CK5QAAAAMAAJ)". Typically, such a system would consists of many expert defined instructions of the exact operations of the AI agent at all the possible situations. Expert systems are indeed similar to "the Chinese Room" where the input symbols are processed with pre-defined rules to generate an output. However, AI agent nowadays are completely different from the expert systems. Although symbol manipulation is still at the lowest level of computation, new AI agents are capable of semantic interpretation in its upper level reasonings. For example, in GPT-3 the text generator, each word is mapped to a point in a "semantic space" where word that shares similar meaning or related to each other are placed close to each other. It understands "parrots" and "finches" are representing similar animals semantically just as well as we do. Furthermore, even if an AI system operates by manipulating meaningless symbols the system as a whole can still generate meaningful behaviors. Although the man and the instructions in the Chinese Room does not understand Chinese as separate parts, the room as a complete system does. As it is said in "[Artificial Intelligence: A Modern Approach](http://aima.cs.berkeley.edu/)", at the very bottom level, human brains and computers both function by manipulating electronic currents. All computation, reasoning and consciousness exists through a combination of millions and billions of such manipulations. Thus, it there is no fundamental difference between human intelligence and program imitations of it. 

We now established that an AI agent that passes the theoretical Turing test is a perfect imitation of human intelligence. We also know that such system is on the horizon with development of technologies like GPT-3 and AphaGo. One question remains: can a perfect imitation of intelligence be considered as intelligence itself? To answer this question, let's consider an AI agent that can in fact imitate human perfectly. When it is impossible to even differentiate this agent and a human, it seems unreasonable to say one is intelligent and the other is not. In fact, our judgements of all external entities in this world are never based on their true nature. We always judge things based on our surface level observation. In his "[Meditations on First Philosophy](https://yale.learningu.org/download/041e9642-df02-4eed-a895-70e472df2ca4/H2665_Descartes%27%20Meditations.pdf)", Descartes pointed out the possibility that an evil deceiver of "utmost power and cunning has employed all his energies in order to deceive me." This means that the external world can all be an illusion created by such a deceiver, which makes it impossible to be sure we are acquiring true knowledge by observing the external world.  Descartes resolved this doubt by appeal to God being no deceiver which should give us faith observations would lead to truth. However, for atheists, this cannot be satisfactory. However, in reality, everybody, including atheists, still in general accepts that their observations are real. If we are already making judgements of the nature of all the things through appearances already, there is no reason not to do the same for intelligence.

One of the questions that repeatedly appear in discussions of AI is "what is the ultimate difference is between human intelligence and artificial intelligence." People has come up with all sorts of answers, like "ability to create art", "ability to reason", or "having emotions". However, as technology evolves, all these proposed differences are growing less and less significant. Arts, arguments, and even emotions are no-longer exclusively produced by human. We now must accept a future where intelligence can be created from programs. After all, it is human nature to replicate and improve what we seen in the natural world. We saw the the sun shining above and created light bulbs to eliminate darkness in the world. We saw birds flying in the sky and created airplanes that took up above the clouds. We saw fish swimming in water and created ships that took us across the oceans. Artificial intelligence is just like these human creations. It is a technology that is destined to take us into a new future.