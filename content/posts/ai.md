---
title: "AI is coming to get you ... but not in the way you think"
date: 2022-10-01T00:01:12-07:00
draft: true
---

In the 2017 documentary [AlphaGo](https://www.youtube.com/watch?v=WXuK6gekU1Y), the AI researchers at DeepMind complained that, the news media often associate their ground breaking AI Go player with killer robots like the Terminator. They think this association contributes to the public's irrational fear of their work on artificial intelligence. Their frustration is very understandable since DeepMind's AlphaGo couldn't be more different from the Terminator. For starters, AlphaGo doesn't look like Arnold Schwarzenegger and couldn't turn itself into liquid metal. Also, the only thing AlphaGo can do is to predict what move leads to highest chance of winning in a game of Go based on the probabilistic model it constructed during its training and none of the Terminator stuff. It couldn't even say "I'll be back" at the end of the game. 

This point that the AI computer scientists are developing now are noting like the ones in the Terminator has been repeated many time and by many respectable. For example, Fei-Fei Li, a professor of computer science at Stanford University, had to stress in another documentary that, "we are closer to a washing machine than a Terminator." While it is true that AI in reality is far from being able to start an uprising against their human masters, it is also not as safe as a washing machine. Their danger lies in the data being collected to train them.

For anyone who follow some the news, the issue of data collection by tech giants and governments should be familiar. We have heard so much about [Google's data farms](https://en.wikipedia.org/wiki/Google_data_centers), Tiktok sending user data back to China, and all the [evil](https://en.wikipedia.org/wiki/Cambridge_Analytica) Facebook has done. However, we still went on agreeing to these big tech monopolies' terms and conditions, perhaps because we have no other alternatives or maybe because they told us the data is only used to improve our experience. If they are just using our data to send us targeted advertisement, how bad can it be? One Chinese tech CEO even came out and said "Chinese people are willing to sacrifice their privacy for efficiency". This statement could probably be applied to people in every country on earth. We just don't fear mishandling of data as much as a robot uprising. Well, we should fear it. Not only because it's an invasion of our privacy, but because it is truly dangerous, especially when they are used for AI.

Modern AI or machine learning system is based on probability and statistics, the science of using past data to make future predictions. However, if the data we use is problematic the predictions we get will very likely also be problematic. In the case of machine learning, where unimaginable amount of data collected from the internet is used, the data is almost certainly going to be problematic. Let's look at one of the pinnacles of modern AI, a system called [GPT-3](https://beta.openai.com/examples/default-qa). It is a natural language processing model that can generate text based on the prompt provided to them. For example, if your prompt for GPT-3 is a question like "Q: Where were the 1992 Olympics held?", it will answer "A: The 1992 Olympics were held in Barcelona, Spain." GPT-3 has shown great ability in language generation. It even published a [opinion piece](https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3) on the guardian explaining why we should not fear AI. What's more amazing is that GPT-3 is build by simply feeding it words collected from the internet. However, the internet contains lots of problematic words and ideas which GPT-3 seem to pick up. Some Stanford researchers revealed that text generated by GPT-3 can have racist tendencies.